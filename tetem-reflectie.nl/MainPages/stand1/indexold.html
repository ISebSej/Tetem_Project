<!DOCTYPE html>
<html lang="nl">
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <!-- Title -->
    <title>Understands - Tetem Reflectie opstelling</title>

    <!--- Set Favicon .UT -->
    <link
      rel="icon"
      type="image/png"
      href="https://1348661504.rsc.cdn77.org/.publisher/sd/utwente_base/ws2016/img/favicons/favicon-16x16.png"
      sizes="16x16"
    />
    <link
      rel="icon"
      type="image/png"
      href="https://1348661504.rsc.cdn77.org/.publisher/sd/utwente_base/ws2016/img/favicons/favicon-32x32.png"
      sizes="32x32"
    />
    <link
      rel="apple-touch-icon"
      sizes="180x180"
      href="https://1348661504.rsc.cdn77.org/.publisher/sd/utwente_base/ws2016/img/favicons/apple-touch-icon.png"
    />

    <!-- Include UT font -->
    <style>
      canvas {
        width: 100%;
        height: auto;
      }
    </style>
    <style>
      :root {
        --main-color: white !important;
        --text-color: #cf53ec !important;
        --select-color: #cf53ec !important;
        --unselect-color: #903aa5 !important;
      }
    </style>
    <!-- Bootstrap core CSS -->
    <link href="../vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet" />
    <!-- Page Stylesheet -->
    <link rel="stylesheet" href="../resources/index.css" />
    <!-- Awesome font 4 icon package -->
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"
    />
  </head>
  <body>
    <!-- Stand identifier -->
    <span id="stand" hidden>1</span>

    <button
      class="tablink"
      onclick="openPage('p1', this, 'var(--select-color)')"
      id="defaultOpen"
    >
      <b class="nl">Verbinden</b>
      <b class="en">Connect</b>
    </button>
    <button
      class="tablink"
      onclick="openPage('p2', this, 'var(--select-color)')"
    >
      <b class="nl">Introductie</b>
      <b class="en">Introduction</b>
    </button>
    <button
      class="tablink"
      onclick="openPage('p3', this, 'var(--select-color)')"
    >
      <b class="nl">Wie ben ik?</b>
      <b class="en">Who am I?</b>
    </button>
    <button
      class="tablink"
      onclick="openPage('p4', this, 'var(--select-color)')"
    >
      <b class="nl">Toestemming</b>
      <b class="en">Consent</b>
    </button>
    <button
      class="tablink"
      onclick="openPage('p5', this, 'var(--select-color)')"
    >
      <b class="nl">Reflectie vragen</b>
      <b class="en">Reflection questions</b>
    </button>

    <div id="p1" class="tabcontent">
      <h2 id="ID">
        <i id="connicon" aria-hidden="true" class="fa fa-user-times"></i>
        <b>Koppelcode: </b>
      </h2>
      <br />
      <p>
        [NL] Hierboven vindt je de koppelcode om uw tablet aan dit scherm te
        koppelen. <br />
        Voer deze in op je tablet en wacht tot de besturing zichtbaar is.
      </p>
      <br />
      <p>
        [EN] Here you can find the connection code to connect your tablet to the
        display <br />
        Enter this on your tablet and wait for the devices to connect
      </p>
    </div>

    <div id="p2" class="tabcontent">
      <div class="nl">
        <h3>Introductie</h3>
        <br />
        <p>
          "Wie ben ik?" introduceert het hoofdthema van "Reflecties". Deze
          installatie weerspiegelt ons letterlijk door het gebruik van
          ouderwetse technologie (met behulp van spiegels), alsook door
          AI-technologie die verschillende filters toepast op de foto die van
          jou wordt genomen door een camera. We zullen je vragen te vertellen
          wie de echte jij is?
        </p>

        <p>
          <b>Filosofisch idee achter deze installatie:</b> Technologie heeft
          altijd onze behoeften, mogelijkheden, ambities en limieten
          weerspiegeld. Met en door technologie construeren -en
          herdefini&euml;ren we- wie we zijn. Met de opkomst van digitale en
          AI-technologie&euml;n wordt het echter steeds ingewikkelder om te
          begrijpen wat het betekent om 'echt' mens te zijn, en om je eigen
          identiteit te kennen. Met deze installatie bieden we een moment om
          (letterlijk) na te denken over technologie en hoe we ons daarmee
          identificeren.
        </p>
      </div>
      <div class="en">
        <h3>Introduction</h3>
        <br />
        <p>
          Installation 1, Look lets you look at yourself through different
          mirrors and filters &ndash; and asks you the question of who you are
          and what your identity is.
        </p>
        <p>
          &ldquo;Who am I?&rdquo; introduces the main theme of
          &ldquo;Reflections&rdquo; by proposing an installation to reflect
          ourselves and upon ourselves. Technology has always mirrored our
          needs, potentials, aspirations and limits. It is with and through
          technology that we construct - and at the same time redefine - our
          identity. But how does the perception of our own identity change when
          AI technologies tell us who we are? How do we relate to our
          &lsquo;self&rsquo; when our preferences, ages and even emotions are
          measured and displayed in front of us? It is increasingly complicated
          to understand what it means to be human in the digital age, and AI
          technologies put us in front of several significant questions. Human
          dignity and its worth are continuously questioned, especially when AI
          seems to become more and more human. What should humans become then?
          With this installation, we offer a moment to (literally)
          reflect.Before proceeding we need to ask your permission in order to
          anonymously collect your answers, and to show them (anonymously) in
          the last installation. This is called informed consent. If you do not
          give permission, you will still be able to see the reflection
          questions for each installation, but you will not be able to submit
          any answers.<br />
        </p>
        <p>
          In this exhibition you will be asked a number of multiple choice
          questions per installation, usually with an open-ended question at the
          end. We collect these questions to display them anonymously on the
          screen in installation 8.<br />
          Your answers will not be used for any other purposes, and will be
          stored according to the standards for secure data storage.<br />
        </p>
      </div>
    </div>

    <div id="p3" class="tabcontent">
      <h3 class="nl">Op wie lijk jij het meest?</h3>
      <h3 class="en">Who do you look like the most</h3>
      <video
        class="col-md-6"
        id="videoInput"
        alt="No Image"
        hidden="true"
      ></video>
      <br />
      <div class="container-fluid" id="videoContainer">
        <div class="row">
          <div class="col">
            <button
              onclick="console.log('hello')"
              id="img1"
              class="btn btn-primary marfix col col-4"
            >
              <div>
                <h2 class="nl">Picture 1</h2>
                <h2 class="en">Picture 1</h2>
                <canvas
                  id="canvasOutput"
                  class="embed-responsive-item"
                ></canvas>
              </div>
            </button>
            <button id="img2" class="btn btn-primary marfix col col-4">
              <div>
                <h2 class="nl">Picture 2</h2>
                <h2 class="en">Picture 2</h2>
                <canvas
                  id="canvasOutput2"
                  class="embed-responsive-item"
                ></canvas>
              </div>
            </button>
            <button id="img3" class="btn btn-primary marfix col col-4">
              <div>
                <h2 class="nl">Picture 3</h2>
                <h2 class="en">Picture 3</h2>
                <canvas
                  id="canvasOutput3"
                  class="embed-responsive-item"
                ></canvas>
              </div>
            </button>
          </div>
        </div>
      </div>
    </div>

    <div id="p4" class="tabcontent">
      <div class="nl">
        <h3>Toestemming</h3>
        <p id="access1">
          Alvorens verder te gaan dienen we jouw toestemming te vragen voor het
          anoniem verzamelen van jouw antwoorden, en om deze (anoniem) te tonen
          in de laatste installatie. Als jij deze toestemming niet geeft, zul je
          per installatie wel de reflectievragen kunnen zien, maar geen
          antwoorden kunnen invoeren.
        </p>
        <p id="access2">
          In deze tentoonstelling worden per installatie een aantal meerkeuze
          vragen aan u gesteld, met tot slot meestal een open vraag. Wij
          verzamelen deze vragen om deze anoniem te tonen op het scherm in
          installatie 8. Uw antwoorden zullen voor geen enkel ander doel worden
          gebruikt, en opgeslagen worden volgens de standaardnormen voor veilige
          dataopslag. Geef op de tablet aan of U hier toestemming voor geeft.
        </p>
        <p id="access3" hidden>
          In het kader van ons project Man and Machine &ndash; Learning in the
          Digital Society zouden wij graag gebruik maken van uw anonieme
          antwoorden voor verder filosofisch onderzoek binnen de vakgroep
          Filosofie van de Universiteit Twente. In dat onderzoek willen we beter
          zicht krijgen op opvattingen van burgers over AI in de digitale
          samenleving. Uw gegevens worden anoniem opgeslagen volgens de
          standaarden voor veilige dataopslag bij het BMS-lab van de
          Universiteit Twente. Voor dat onderzoek hebben wij een aantal extra
          gegevens nodig. Als u akkoord gaat met gebruik voor onderzoek, wilt u
          dan onderstaande gegevens invullen op de tablet?
        </p>
      </div>
      <div class="en">
        <h3>Consent</h3>
        <p>
          Before proceeding we need to ask your permission in order to
          anonymously collect your answers, and to show them (anonymously) in
          the last installation. This is called informed consent. If you do not
          give permission, you will still be able to see the reflection
          questions for each installation, but you will not be able to submit
          any answers.
        </p>
        <p>
          In this exhibition you will be asked a number of multiple choice
          questions per installation, usually with an open-ended question at the
          end. We collect these questions to display them anonymously on the
          screen in installation 8. Your answers will not be used for any other
          purposes, and will be stored according to the standards for secure
          data storage.
        </p>
      </div>
    </div>

    <div id="p5" class="tabcontent">
      <div class="nl">
        <h3>Reflectie vragen</h3>
        <p>Als je klaar bent kun je verder gaan naar de vragenlijst</p>
      </div>
      <div class="en">
        <h3>Reflection questions</h3>
        <p>Once you're done you'll be forwarded to the reflection questions</p>
      </div>
    </div>

    <!-- Tabbing of page -->
    <script type="text/javascript" src="../resources/tab.js"></script>

    <!-- PeerJS library -->
    <script src="https://unpkg.com/peerjs@1.3.1/dist/peerjs.min.js"></script>

    <!-- PeerJS WebRTC configuration -->
    <script type="text/javascript" src="../resources/peerJSreceive.js"></script>

    <!-- YouTube API -->
    <script>
      // 2. This code loads the IFrame Player API code asynchronously.
      var tag = document.createElement("script");

      tag.src = "https://www.youtube.com/iframe_api";
      var firstScriptTag = document.getElementsByTagName("script")[0];
      firstScriptTag.parentNode.insertBefore(tag, firstScriptTag);

      // 3. This function creates an <iframe> (and YouTube player)
      //    after the API code downloads.
      function onYouTubeIframeAPIReady() {
        window.player = new YT.Player("player", {
          height: "720",
          width: "1280",
          videoId: "P4JpD1PWBDI",
          events: {
            onStateChange: onPlayerStateChange,
          },
        });
      }

      function onPlayerStateChange(event) {
        if (event.data == YT.PlayerState.ENDED) {
          console.log("--- Video Done!");
          window.conn.send("Done");
        }
      }
    </script>
    <!-- Load cv library -->
    <script src="../resources/opencvExp.js" type="text/javascript"></script>
    <script src="https://docs.opencv.org/master/utils.js"></script>

    <script>
      // Entry point for video processing
      var canvasFrame = document.getElementById("canvasOutput"); // canvasFrame is the id of <canvas>
      var context = canvasFrame.getContext("2d");
      // Import opencv js module from local resources
      // original source "https://docs.opencv.org/master/opencv.js"

      //wait for opencv to load before initalizing runtime
      cv["onRuntimeInitialized"] = () => {
        // find camera and set to <video /> object

        myinitVideoCamera
          .then((size) => {
            // get webcam information
            let video = document.getElementById("videoInput");

            // initialize cascades for face detection
            let classifier = new cv.CascadeClassifier();
            // http get haar cascade file
            let utils = new Utils("errorMessage"); //use utils class

            let faceCascadeFile = "./haarcascade_frontalface_default.xml"; // path to xml

            try {
              // use createFileFromUrl to "pre-build" the xml
              utils.createFileFromUrl(faceCascadeFile, faceCascadeFile, () => {
                classifier.load(faceCascadeFile); // in the callback, load the cascade from file
              });
            } catch (err) {
              console.log("error");
              console.log(cv.exceptionFromPtr(err).msg);
            }

            let faces = new cv.RectVector();

            // preallocate source memory
            let imgData = context.getImageData(
              0,
              0,
              size.camWidth,
              size.camHeight
            );
            let src = cv.matFromImageData(imgData);

            // preallocate destination memory
            let dst = new cv.Mat(size.camHeight, size.camWidth, cv.CV_8UC4);
            let dst2 = new cv.Mat(size.camHeight, size.camWidth, cv.CV_8UC4);
            let dst3 = new cv.Mat(size.camHeight, size.camWidth, cv.CV_8UC4);
            let gray = new cv.Mat(size.camHeight, size.camWidth, cv.CV_8UC4);
            // create VideoCapture object
            let cap = new cv.VideoCapture(video);

            // Wrap all object up a data file and pass it to the main video processing function
            let data = {
              source: src,
              destination: dst,
              destination2: dst2,
              destination3: dst3,
              gray: gray,
              capture: cap,
              width: size.camWidth,
              height: size.camHeight,
              faces: faces,
              classif: classifier,
            };

            setTimeout(processVideo.bind(data), 1000);
          })
          .catch((err) => {
            console.log(err);
          });
      };

      // Define Promise to initialize the webcam
      let myinitVideoCamera = new Promise((resolve, reject) => {
        navigator.mediaDevices
          .getUserMedia({ video: true, audio: false })
          //set <video /> object to stream
          .then(function (stream) {
            // video is the id of video tag
            let video = document.getElementById("videoInput");
            // Attach webcam stream to video object
            video.srcObject = stream;
            video.play();
            //Find webcam parameters
            let { width, height } = stream.getTracks()[0].getSettings();
            video.width = width;
            video.height = height;
            // Return webcam parameters and continue to .then()
            resolve({ camWidth: width, camHeight: height });
          })
          // error handling
          .catch(function (err) {
            reject(err);
          });
      });

      // Main processing function
      // draws frams -> imshow to its canvas -> schedule next processing event
      function processVideo() {
        // Start timer to maintain FPS
        let begin = Date.now();
        // Get raw webcam frame and save it in source
        this.capture.read(this.source);
        this.source.copyTo(this.destination);
        // Processing of Canvas 1 - Top Left
        //cv.cvtColor(this.source, this.destination, cv.COLOR_RGBA2GRAY);
        cv.cvtColor(this.source, this.gray, cv.COLOR_RGBA2GRAY, 0);
        // detect faces.
        try {
          let tik = Date.now();
          this.classif.detectMultiScale(
            this.gray,
            this.faces,
            1.3,
            3,
            0,
            new cv.Size(75, 75)
          );
          let tok = Date.now() - tik;
          console.log(tok);
        } catch (err) {
          console.log("error");
          console.log(cv.exceptionFromPtr(err).msg);
        }

        // draw this.faces.
        for (let i = 0; i < this.faces.size(); ++i) {
          let face = this.faces.get(i);
          let point1 = new cv.Point(face.x, face.y);
          let point2 = new cv.Point(face.x + face.width, face.y + face.height);
          cv.rectangle(this.destination, point1, point2, [255, 0, 0, 255]);
        }

        cv.imshow("canvasOutput", this.destination);

        // Processing of Canvas 2 - Top Right
        //cv.cvtColor(this.source, this.destination2, cv.COLOR_RGBA2GRAY);
        // cv.threshold(this.source, this.destination2, 177, 200, cv.THRESH_BINARY);
        cv.imshow("canvasOutput2", this.destination);

        // Processing of Canvas 3 - Top Right
        //cv.cvtColor(this.source, this.destination3, cv.COLOR_RGBA2GRAY);
        // let src = new cv.Mat();
        // cv.cvtColor(this.source, src, cv.COLOR_RGBA2RGB, 0);
        // cv.bilateralFilter(src, this.destination3, 3, 400, 75, cv.BORDER_DEFAULT);
        // src.delete();
        cv.imshow("canvasOutput3", this.destination);

        // Schedule next processing event
        let delay = 1000 / 30 - (Date.now() - begin);
        setTimeout(processVideo.bind(this), delay);
      }
    </script>

    <script>
      // console.log("POST TEST");
      // var http = new XMLHttpRequest();
      // var url = "https://tetem-reflectie.nl:3000/visitors";
      // var params = "agreement=1&test_usr=1&tablet_num=234235";
      // http.open("POST", url, true);

      // //Send the proper header information along with the request
      // http.setRequestHeader("Content-type", "application/x-www-form-urlencoded");

      // http.onreadystatechange = function () {
      //   //Call a function when the state changes.
      //   if (http.readyState == 4 && http.status == 200) {
      //     alert(http.responseText);
      //   }
      // };
      // http.send(params);
    </script>
  </body>
</html>
